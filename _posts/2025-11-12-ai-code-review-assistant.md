

---
layout: post
title: "AI Code Review Assistant â€” LLM + RAG with OpenAI & Pinecone"
date: 2025-11-12
categories: [AI, LLM, RAG, Pinecone, OpenAI]
---

## ğŸš€ Project Overview

As part of my learning journey in AI integrations, I built a full-stack **AI Code Review Assistant** â€” a tool that analyzes source code, detects security or performance issues, and suggests automated fixes.

It uses:
- **OpenAI GPT models** for reasoning and feedback generation  
- **Pinecone** for vector storage and similarity search (RAG)  
- **React + Tailwind** for the frontend  
- **FastAPI / Express (backend)** for handling requests and prompt orchestration  

---

## âš™ï¸ Architecture


- The **frontend** lets users paste code and click â€œAnalyzeâ€ or â€œFix.â€  
- The **backend** validates input and sends it to OpenAI, optionally enriching context from Pinecone.  
- The **RAG pipeline** retrieves semantically similar code patterns to ground AI responses in realistic examples.  

---

## ğŸ§  Retrieval-Augmented Generation (RAG)

To improve accuracy and reduce hallucinations:
- I stored embeddings of known code â€œpatternsâ€ (e.g., SQL injection, unsafe concatenation, missing error handling) in Pinecone.
- When new code is submitted, itâ€™s embedded using OpenAIâ€™s `text-embedding-3-small` model.
- Pinecone returns the top-k most similar snippets.
- These retrieved examples are added to the OpenAI prompt for better reasoning and contextual review.

This gives the model real world grounding before generating suggestions, similar to how a senior engineer references prior code reviews.

---

## ğŸ”Œ Backend Endpoints

```http
POST /analyze
Body: { code, language, filename }
Resp: { ok, review: { raw, structured } }

POST /fix
Body: { code, language, filename }
Resp: { ok, code }

ğŸ’¡ Learnings and Takeaways
ğŸ§© 1. AI Integration Patterns

LLMs alone are stateless, but connecting them with a vector database (Pinecone) transforms them into a reasoning system with memory.
Building a small RAG pipeline gave me deeper understanding of embedding storage, similarity search, and context injection.

ğŸ§  2. Prompt Engineering Matters

Different instructions drastically changed the quality of suggestions.
Adding â€œexplain like a senior engineer reviewing production codeâ€ improved tone and structure.

âš¡ 3. End-to-End Ownership

I built both frontend and backend â€” integrating Tailwind, React hooks (useState, useEffect), and asynchronous fetch requests to verify system health.
Debugging full-stack OpenAI + Pinecone integration taught me how to trace data flow and manage environment variables securely.

ğŸª„ Future Improvements

- Deploy backend to Render or AWS Lambda

- Add user authentication for personalized context memory

- Implement LLM feedback fine-tuning based on user-rated responses

ğŸ§¾ Conclusion

This project helped me explore LLM integration engineering â€” connecting AI reasoning models with external data sources for intelligent automation.
It reinforced how RAG pipelines, when implemented thoughtfully, can make AI assistants context-aware and highly reliable.